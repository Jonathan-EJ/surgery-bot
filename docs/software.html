<!doctype html>
<html>
  <head>
    <link rel="stylesheet" href="software.css">
    <title>C106A Fall 2021 Project</title>
  </head>
  <body class="background">
    <div class="wrap"> 
      <div class="nav">
        <div class="title"> 
            <a href="index.html">Surgery Robot</a>
        </div>
        <div class="navbar"> 
          <a href="design.html">DESIGN</a>
          <a href="implementation.html">IMPLEMENTATION</a>
          <a href="demo.html">DEMO</a>
          <a href="conclusion.html">CONCLUSION</a>
          <a href="team.html">TEAM</a>
          <a href="resources.html">RESOURCES</a>
        </div>
      </div>
      <div class="content"> 
        <div class="header"> 
          <h2>Software</h2>
        </div>
        <div class="content-header">
          <h3>What's included</h3>
          <div class="column-container">
            <div class="column">
              <h4>Planning Package</h4>
              <p>Contains code to move Baxter's arms to perform suturing</p>
            </div>
            <div class="column">
              <h4>Vision Package</h4>
              <p>Contains code to project Realsense camera's pointcloud onto Baxter's base frame and image segmentation code</p>
            </div>
          </div>
        </div>
        <div class="content-box">
          <h3>Planning</h3>
          <div class="column-container">
            <div class="column">
              <img src="media/images/flow chart.png">
            </div>
            <div class="column">
              <p>To start up the Surgery Robot, run_planning.launch and run_vision.launch is run. 
                These launch files start up the necessary nodes such as the trajectory action server and the Realsense camera.
              </p>
              <p>After the required nodes are up, suture.py is run.</p>
              <p>The robot proceeds to calibrate its grippers and ask the user to place the needle into its grip and to ready the patient in its workspace.</p>
              <p>The robot then assumes its rest position and ask the user for the number of sutures the user would like the robot to perform.</p>
              <p>The user then selects the desired points of entry in RVIZ using the Publish Point Tool.</p>
              <p>The the robot proceeds to suture.</p>
            </div>
          </div>
          <br>
          <hr>
          <br>
          <div class="column-container">
            <div class="column">
              <div class="suture-photos">
                <img src="media/images/suture0.jpg">
                <img src="media/images/suture1.jpg">
                <img src="media/images/suture2.jpg">
                <img src="media/images/suture3.jpg">
              </div>
            </div>
            <div class="column">
              <p>To generate the suture trajectories, 6 key poses are used.
                These poses are grouped into 2 set: entry and exit.</p>
              <p>The entry set consists of the entry, insert, and release poses.
                The entry pose defines the pose of before the needle is inserted into the patient.
                The insert pose defines the pose of needle insertion.
                The release pose defines the intermediary pose before the robot returns its rest pose.
                These pose are used exclusively by the left gripper.</p>
              <p>The exit set consists of the entry, insert, and release poses.
                The grip pose defines the pose for the gripper to interact with the needle.
                The exit pose defines the pose where the needle completely exits the patient.
                The return pose defines the intermediary pose before the robot returns its rest pose.
                These pose are used exclusively by the right gripper.</p>
              <p>The rest pose the robot assumes is used to hand over the needle before performing the next suture.
                This a sideways pose was chosen to provide more clearance since the Baxter wrist are quite large.</p>
            </div>
          </div>
          <br>
        </div>
        <div class="content-box">
          <h3>Vision</h3>
            <img src="media/images/vision_flow.PNG">
            <p>The vision component of our software includes running two launch files.</p>
            <ul>
              <li>The rs_camera.launch is a custumized version of the realsense rs_camera.launch. 
                This starts the realsense nodes and starts sending messages through different topics such as RGB, Pointcloud...etc.</li>
              <li>The ar_track_realsense.launch file integrates lab 4 (AR Tracking) with a realsense camera. 
                This launch file takes care of the frame transformation between realsense and baxter frame after calibration.</li>
            </ul>
            <p>After these launch files are run, running main.py will add additional topics such as \segmented_workspace which is a segmented image of the surgery robots workspace. 
              This will get rid of unnecessary noise and only keep the patient model and the table.</p>
            <div class="img-box">
              <p>Raw Image</p>
              <img src="media/images/Screenshot_at_2021-12-13_18_33_51.png">
            </div>
            <div class="img-box">
              <p>Segmented Image</p>
              <img src="media/images/Screenshot_at_2021-12-13_18_33_14.png">
            </div>

            <br>
            <hr>
            <br>
            <p>Note: After the presentation, we built a CNN model for image segmentation. 
              Despite our limited training data set, we were able to get by relatively well. 
              However, we were not able to test out our CNN model live due to the unavailability of the torchvision on lab computers.</p>
            <p>We strongly believe this model has the potential to get a more reliable segmented pointcloud in Rviz.</p>
            <div class="img-box">
              <p>Ground Truth</p>
              <img src="media/images/ground_truth.png">
            </div>
            <div class="img-box">
              <p>Predicted by our CNN model. Edge artifacts, perhaps due to lack of thousands of training images. </p>
              <img src="media/images/prediction.png">
            </div>
            <div class="img-box">
              <p>Canny edge detection</p>
              <img src="media/images/canny_edge.jpg">
            </div>
            <div class="img-box">
              <p>Prediction subtracted with Gaussian blurred edges gives a very good final segmentation.</p>
              <img src="media/images/blurred_prediction.jpg">
            </div>
        </div>
      </div>
      <div class="link">
        <a href="hardware.html">Hardware></a>
      </div> 
      <div class="footer">
        <h3>EE106A Final Project Fall 2021</h3>
    </div>
  </body>
</html>
